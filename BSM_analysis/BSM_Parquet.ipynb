{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bded5c5",
   "metadata": {},
   "source": [
    "# Data Dictionary Generation\n",
    "Analyze the actual Parquet file to understand the data structure and generate an accurate data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5028cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC INFO ===\n",
      "Shape: (1, 6)\n",
      "Columns: ['mf_bytes', 'TimeStamp', 'MessageType', 'Geohash', 'Latitude', 'Longitude']\n",
      "\n",
      "=== DATA TYPES ===\n",
      "mf_bytes        object\n",
      "TimeStamp      float64\n",
      "MessageType     object\n",
      "Geohash         object\n",
      "Latitude       float64\n",
      "Longitude      float64\n",
      "dtype: object\n",
      "\n",
      "=== SAMPLE DATA (First Row) ===\n",
      "mf_bytes: <bytes, length=512>\n",
      "TimeStamp: 1756415644.563512\n",
      "MessageType: BasicSafetyMessage\n",
      "Geohash: wx4dyyrk\n",
      "Latitude: 39.8934872\n",
      "Longitude: 116.3223736\n",
      "\n",
      "=== DETAILED COLUMN ANALYSIS ===\n",
      "\n",
      "mf_bytes:\n",
      "  Type: object\n",
      "  Non-null: 1/1 (100.0%)\n",
      "  Unique values: 1\n",
      "  Data type: Binary/bytes\n",
      "  Byte lengths: [512]\n",
      "\n",
      "TimeStamp:\n",
      "  Type: float64\n",
      "  Non-null: 1/1 (100.0%)\n",
      "  Range: 1756415644.563512 to 1756415644.563512\n",
      "  Sample values: [np.float64(1756415644.563512)]\n",
      "\n",
      "MessageType:\n",
      "  Type: object\n",
      "  Non-null: 1/1 (100.0%)\n",
      "  Unique values: 1\n",
      "  Values: ['BasicSafetyMessage']\n",
      "\n",
      "Geohash:\n",
      "  Type: object\n",
      "  Non-null: 1/1 (100.0%)\n",
      "  Unique values: 1\n",
      "  Values: ['wx4dyyrk']\n",
      "\n",
      "Latitude:\n",
      "  Type: float64\n",
      "  Non-null: 1/1 (100.0%)\n",
      "  Range: 39.8934872 to 39.8934872\n",
      "  Sample values: [np.float64(39.8934872)]\n",
      "\n",
      "Longitude:\n",
      "  Type: float64\n",
      "  Non-null: 1/1 (100.0%)\n",
      "  Range: 116.3223736 to 116.3223736\n",
      "  Sample values: [np.float64(116.3223736)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the parquet file\n",
    "file_path = 'parquet/v2x_messages_20250828_221403_846781.parquet'\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "print('=== BASIC INFO ===')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "print()\n",
    "\n",
    "print('=== DATA TYPES ===')\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "print('=== SAMPLE DATA (First Row) ===')\n",
    "for col in df.columns:\n",
    "    sample_val = df[col].iloc[0] if len(df) > 0 else 'N/A'\n",
    "    # Handle potential binary data safely\n",
    "    if isinstance(sample_val, bytes):\n",
    "        print(f'{col}: <bytes, length={len(sample_val)}>')\n",
    "    else:\n",
    "        print(f'{col}: {sample_val}')\n",
    "print()\n",
    "\n",
    "print('=== DETAILED COLUMN ANALYSIS ===')\n",
    "for col in df.columns:\n",
    "    print(f'\\n{col}:')\n",
    "    print(f'  Type: {df[col].dtype}')\n",
    "    non_null_count = df[col].notna().sum()\n",
    "    print(f'  Non-null: {non_null_count}/{len(df)} ({non_null_count/len(df)*100:.1f}%)')\n",
    "    \n",
    "    if df[col].dtype == 'object':\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f'  Unique values: {unique_count}')\n",
    "        \n",
    "        # Check if it's binary data\n",
    "        sample_vals = df[col].dropna().head(3)\n",
    "        has_bytes = any(isinstance(val, bytes) for val in sample_vals)\n",
    "        \n",
    "        if has_bytes:\n",
    "            print(f'  Data type: Binary/bytes')\n",
    "            # Show byte lengths\n",
    "            byte_lengths = [len(val) if isinstance(val, bytes) else 0 for val in sample_vals]\n",
    "            print(f'  Byte lengths: {byte_lengths}')\n",
    "        else:\n",
    "            # Regular string analysis\n",
    "            if unique_count <= 15:\n",
    "                print(f'  Values: {sorted(df[col].dropna().unique())}')\n",
    "            else:\n",
    "                print(f'  Sample values: {list(df[col].dropna().unique()[:5])}')\n",
    "                \n",
    "    elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "        if non_null_count > 0:\n",
    "            print(f'  Range: {df[col].min()} to {df[col].max()}')\n",
    "            print(f'  Sample values: {list(df[col].dropna().head(3).values)}')\n",
    "    elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "        if non_null_count > 0:\n",
    "            print(f'  Range: {df[col].min()} to {df[col].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TIMESTAMP CONVERSION ===\n",
      "Timestamp 1756415644.563512 = 2025-08-28 14:14:04.563512 UTC\n",
      "\n",
      "=== COORDINATE ANALYSIS ===\n",
      "Location: 39.8934872, 116.3223736\n",
      "This appears to be in Beijing, China area\n",
      "\n",
      "=== MESSAGE BYTES ANALYSIS ===\n",
      "Message bytes length: 512\n",
      "First 20 bytes (hex): 0014250142317f276e3fe6b6162c584f9a5b8800\n",
      "\n",
      "=== SCHEMA COMPATIBILITY ===\n",
      "✓ mf_bytes: Binary message data (matches notebook expectation)\n",
      "✓ TimeStamp: Unix timestamp as float64\n",
      "✓ Geohash: String for spatial indexing\n",
      "✓ MessageType: String identifier\n",
      "✓ Latitude/Longitude: Float64 coordinates in decimal degrees\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis for data dictionary\n",
    "print('=== TIMESTAMP CONVERSION ===')\n",
    "timestamp_val = df['TimeStamp'].iloc[0]\n",
    "from datetime import datetime\n",
    "dt = datetime.fromtimestamp(timestamp_val)\n",
    "print(f'Timestamp {timestamp_val} = {dt} UTC')\n",
    "\n",
    "print('\\n=== COORDINATE ANALYSIS ===')\n",
    "lat, lon = df['Latitude'].iloc[0], df['Longitude'].iloc[0]\n",
    "print(f'Location: {lat}, {lon}')\n",
    "\n",
    "\n",
    "print('\\n=== MESSAGE BYTES ANALYSIS ===')\n",
    "msg_bytes = df['mf_bytes'].iloc[0]\n",
    "print(f'Message bytes length: {len(msg_bytes)}')\n",
    "print(f'First 20 bytes (hex): {msg_bytes[:20].hex()}')\n",
    "\n",
    "# This matches the notebook expectation for 'mf_bytes' field\n",
    "print('\\n=== SCHEMA COMPATIBILITY ===')\n",
    "print('✓ mf_bytes: Binary message data (matches notebook expectation)')\n",
    "print('✓ TimeStamp: Unix timestamp as float64') \n",
    "print('✓ Geohash: String for spatial indexing')\n",
    "print('✓ MessageType: String identifier')\n",
    "print('✓ Latitude/Longitude: Float64 coordinates in decimal degrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4b030",
   "metadata": {},
   "source": [
    "# BSM Parquet Analysis\n",
    "\n",
    "This notebook processes and analyzes Basic Safety Message (BSM) data stored in Parquet files. It demonstrates how to load, filter, decode, and visualize BSM messages, focusing on specific geohashes and vehicle movement over time.\n",
    "\n",
    "The workflow includes:\n",
    "- Loading and concatenating Parquet files\n",
    "- Time and geohash-based grouping\n",
    "- Focusing on specific geohashes\n",
    "- Decoding BSM messages using a C binary\n",
    "- Extracting BSM IDs and timestamps\n",
    "- Identifying repeated BSMs\n",
    "- Visualizing vehicle movement with Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d572cc",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "This cell imports essential Python libraries for data manipulation, visualization, and file handling, including pandas, matplotlib, seaborn, pyarrow, and os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa6c76",
   "metadata": {},
   "source": [
    "## Load and Concatenate Parquet Files\n",
    "This cell locates all Parquet files in the specified directory, reads them into pandas DataFrames, concatenates them into a single DataFrame, and displays basic information about the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Set the directory containing your parquet files\n",
    "parquet_dir = \"./parquet\"  # Update if needed\n",
    "\n",
    "# Find all .parquet files in the directory\n",
    "parquet_files = sorted(glob.glob(os.path.join(parquet_dir, \"*.parquet\")))\n",
    "\n",
    "print(f\"Found {len(parquet_files)} Parquet files.\")\n",
    "\n",
    "# Read and concatenate all parquet files into a single DataFrame\n",
    "if parquet_files:\n",
    "\tdf_all = pd.concat([pd.read_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "\t# Show basic info\n",
    "\tdf_all.info()\n",
    "\tdf_all.head()\n",
    "else:\n",
    "\tprint(\"No Parquet files found in the directory.\")\n",
    "\tdf_all = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd45d1f",
   "metadata": {},
   "source": [
    "## Group by Time and Geohash\n",
    "This cell converts timestamps to datetime, buckets them by minute, groups the data by time bucket and geohash, and displays the largest groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ff391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert float timestamp to datetime\n",
    "df_all['Time'] = pd.to_datetime(df_all['TimeStamp'], unit='s')\n",
    "\n",
    "# Optional: round to nearest 1 minute\n",
    "df_all['TimeBucket'] = df_all['Time'].dt.floor('1MIN')  # Change to '1min', '30s' etc. as needed\n",
    "\n",
    "grouped = df_all.groupby(['TimeBucket', 'Geohash'])\n",
    "\n",
    "group_sizes = grouped.size().reset_index(name='Count')\n",
    "group_sizes.sort_values('Count', ascending=False).head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769639b",
   "metadata": {},
   "source": [
    "## Filter for Focus Geohashes\n",
    "This cell filters the DataFrame to include only rows with geohashes of interest, sorts the results for readability, and displays message counts per geohash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18590bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_geohashes = ['9tbq2v6h', '9tbq8b1c', '9tbq8c1g', '9tbq8ccy']\n",
    "df_focus = df_all[df_all['Geohash'].isin(focus_geohashes)]\n",
    "\n",
    "print(\"Message count per geohash:\")\n",
    "print(df_focus['Geohash'].value_counts())\n",
    "\n",
    "# Sort by geohash and time for better readability\n",
    "df_focus_sorted = df_focus.sort_values(by=[\"Geohash\", \"Time\"])\n",
    "df_focus_sorted.reset_index(drop=True, inplace=True)\n",
    "df_focus_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea79c59",
   "metadata": {},
   "source": [
    "## Decode BSM Messages Using C Binary\n",
    "This cell locates the C binary decoder, prepares the focus DataFrame, converts message bytes to hex, and decodes each BSM message using the external decoder. It also prints a few decoded outputs for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Detect repo root and set decoder path RELATIVE to repo root\n",
    "notebook_dir = Path.cwd()\n",
    "repo_root = None\n",
    "\n",
    "# Traverse up to find .git as marker for root\n",
    "for parent in notebook_dir.parents:\n",
    "    if (parent / \".git\").exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "if repo_root is None:\n",
    "    repo_root = notebook_dir  # fallback if not using git\n",
    "\n",
    "DECODER_PATH = repo_root / \"libsm/b2v-libsm/build/bin/decodeToJER\"\n",
    "print(\"Detected repo root:\", repo_root)\n",
    "print(\"Decoder Path:\", DECODER_PATH)\n",
    "if not DECODER_PATH.exists():\n",
    "    raise FileNotFoundError(f\"decodeToJER not found at {DECODER_PATH}\")\n",
    "\n",
    "# 2. Focused geohashes\n",
    "focus_geohashes = ['9tbq2v6h', '9tbq8b1c', '9tbq8c1g', '9tbq8ccy']\n",
    "df_focus = df_all[df_all['Geohash'].isin(focus_geohashes)].copy()\n",
    "print(\"BSMs in focus:\", len(df_focus))\n",
    "\n",
    "# 3. Convert mf_bytes to hex\n",
    "def mf_bytes_to_hex(val):\n",
    "    if isinstance(val, (bytes, bytearray)):\n",
    "        return val.hex()\n",
    "    if isinstance(val, str) and val.startswith(\"b'\"):  # as string repr\n",
    "        return eval(val).hex()\n",
    "    return None\n",
    "\n",
    "df_focus[\"mf_hex\"] = df_focus[\"mf_bytes\"].apply(mf_bytes_to_hex)\n",
    "\n",
    "# 4. Decode each BSM using the C binary\n",
    "def decode_bsm_hex(hex_str):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [str(DECODER_PATH), \"-i\", hex_str],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=True,\n",
    "            text=True,\n",
    "            timeout=3,\n",
    "        )\n",
    "        return result.stdout\n",
    "    except Exception as e:\n",
    "        print(f\"[DecodeError] {e}\")\n",
    "        return None\n",
    "\n",
    "df_focus[\"jer\"] = df_focus[\"mf_hex\"].apply(decode_bsm_hex)\n",
    "print(\"Decoded BSMs:\", df_focus['jer'].notnull().sum())\n",
    "\n",
    "# 5. (Optional) Show a few decoded outputs for inspection\n",
    "for jer in df_focus[\"jer\"].dropna().head(3):\n",
    "    print(jer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e524582",
   "metadata": {},
   "source": [
    "## Extract BSM ID and Identify Repeated Messages\n",
    "This cell parses the decoded JER output to extract BSM IDs and secMark values, sorts by ID and timestamp, computes time differences between consecutive messages with the same ID, and identifies BSMs that are repeated within a short time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 1. Extract id and secMark from the decoded JER string (for each row)\n",
    "def extract_id_secmark(jer_str):\n",
    "    try:\n",
    "        jer = json.loads(jer_str)\n",
    "        bsm = jer[\"value\"][\"BasicSafetyMessage\"][\"coreData\"]\n",
    "        return bsm.get(\"id\"), bsm.get(\"secMark\")\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "df_focus[[\"bsm_id\", \"bsm_secMark\"]] = df_focus[\"jer\"].apply(lambda x: pd.Series(extract_id_secmark(x)))\n",
    "\n",
    "# 2. Check which BSMs have the same id and nearby timestamps (TimeStamp or secMark)\n",
    "# Sort for easier comparison\n",
    "df_focus_sorted = df_focus.sort_values([\"bsm_id\", \"TimeStamp\"])\n",
    "\n",
    "# Compute time difference (in seconds) to previous message with same id\n",
    "df_focus_sorted[\"prev_TimeStamp\"] = df_focus_sorted.groupby(\"bsm_id\")[\"TimeStamp\"].shift(1)\n",
    "df_focus_sorted[\"dt_sec\"] = df_focus_sorted[\"TimeStamp\"] - df_focus_sorted[\"prev_TimeStamp\"]\n",
    "\n",
    "# Show BSMs with dt_sec < threshold (e.g., 2 seconds)\n",
    "threshold = 2\n",
    "nearby = df_focus_sorted[df_focus_sorted[\"dt_sec\"].notnull() & (df_focus_sorted[\"dt_sec\"] < threshold)]\n",
    "\n",
    "print(f\"BSMs with repeated id within {threshold} seconds:\")\n",
    "display(nearby[[\"bsm_id\", \"TimeStamp\", \"dt_sec\", \"Geohash\", \"Latitude\", \"Longitude\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e79867",
   "metadata": {},
   "source": [
    "## Visualize BSM Movement by Vehicle ID\n",
    "This cell prepares the data for animation, then uses Plotly to create an animated map showing the movement of vehicles (by BSM ID) over time, with one frame per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Ensure Time is datetime and round/floor to seconds\n",
    "df_focus_sorted[\"Time\"] = pd.to_datetime(df_focus_sorted[\"Time\"])\n",
    "df_focus_sorted[\"Time_sec\"] = df_focus_sorted[\"Time\"].dt.floor(\"S\")\n",
    "\n",
    "# (Optional) Convert bsm_id to string for display\n",
    "df_focus_sorted[\"bsm_id\"] = df_focus_sorted[\"bsm_id\"].astype(str)\n",
    "\n",
    "# Sort by Time for animation\n",
    "df_anim = df_focus_sorted.sort_values(\"Time_sec\")\n",
    "\n",
    "fig = px.scatter_map(\n",
    "    df_anim,\n",
    "    lat=\"Latitude\",\n",
    "    lon=\"Longitude\",\n",
    "    color=\"bsm_id\",      # Color by vehicle\n",
    "    animation_frame=df_anim[\"Time_sec\"].dt.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    hover_name=\"bsm_id\",\n",
    "    zoom=12,\n",
    "    height=600,\n",
    "    map_style=\"open-street-map\"  # Same style as before\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"BSM Movement by Vehicle ID (Per Second)\",\n",
    "    margin={\"r\":0, \"t\":30, \"l\":0, \"b\":0},\n",
    ")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
